{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'user': ['Jason', 'Molly', 'Amy', 'Jake', 'Amy'],\n",
    "        'doc':['q', 'M', 'T', 'q', 'Ay'],\n",
    "        'author': ['Jason', 'Jason', 'Tina', 'Jason', 'Amy'],\n",
    "        'views': [4, 24, 3100, 2, 3],\n",
    "        'y': [1,1,3,1,1]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(embedding_size, field_vocab_size=[], hidden_units=[4,4,4], dropout=0.5):\n",
    "    F = len(field_vocab_size)\n",
    "    \n",
    "    # prepare embeddings\n",
    "    inputs = []\n",
    "    embed_list = [] \n",
    "    for i, vocab_size in enumerate(field_vocab_size):\n",
    "        in_ = keras.Input(shape=(1,))\n",
    "        inputs.append(in_)\n",
    "        embed_list.append(layers.Embedding(vocab_size, embedding_size)(in_))\n",
    "    embed_list = layers.concatenate(embed_list, axis=1) # none, F, K\n",
    "    \n",
    "    fm_one_inputs = []\n",
    "    embed_one_list = [] # none, F, 1\n",
    "    for i, vocab_size in enumerate(field_vocab_size):\n",
    "        in_ = keras.Input(shape=(1,))\n",
    "        inputs.append(in_)\n",
    "        embed_one_list.append(layers.Embedding(vocab_size, 1)(in_))\n",
    "    fm_first_in = layers.concatenate(embed_one_list, axis=1)\n",
    "    fm_first_in = backend.squeeze(fm_first_in, axis=2) # none, F\n",
    "    \n",
    "    # dense layer\n",
    "    dropouts = [dropout] * len(hidden_units)\n",
    "    weight_init = keras.initializers.glorot_uniform()\n",
    "    \n",
    "    deep_in = layers.Reshape((F*embedding_size,))(embed_list)\n",
    "    for i, (h, d) in enumerate(zip(hidden_units, dropouts)):\n",
    "        z = layers.Dense(units=h, kernel_initializer=weight_init)(deep_in)\n",
    "        z = layers.BatchNormalization(axis=-1)(z)\n",
    "        z = keras.activations.relu(z)\n",
    "        z = layers.Dropout(d,seed=d * i)(z) if d > 0 else z\n",
    "    deep_out = layers.Dense(units=1, activation=tf.nn.softmax, kernel_initializer=weight_init)(z)\n",
    "    # deep_out: None, 1\n",
    "    \n",
    "    # fm layer\n",
    "    fm_first_order = backend.sum(fm_first_in, axis=1) #None, 1\n",
    "    \n",
    "    emb_sum_squared = backend.square(backend.sum(embed_list, axis=1)) #none, K\n",
    "    emb_squared_sum = backend.sum(backend.square(embed_list), axis=1) #none, K\n",
    "    fm_second_order = layers.Subtract()([emb_sum_squared, emb_squared_sum])\n",
    "    fm_second_order = backend.sum(fm_second_order, axis=1) #none, 1\n",
    "    fm_out = layers.Add()([fm_first_order, fm_second_order])\n",
    "    \n",
    "    out = layers.Add()([deep_out, fm_out])\n",
    "    out = layers.Activation(activation='sigmoid')(out)\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_175 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_176 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_177 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_174 (Embedding)       (None, 1, 4)         4           input_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_175 (Embedding)       (None, 1, 4)         8           input_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_176 (Embedding)       (None, 1, 4)         12          input_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 3, 4)         0           embedding_174[0][0]              \n",
      "                                                                 embedding_175[0][0]              \n",
      "                                                                 embedding_176[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 12)           0           concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_178 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_179 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_180 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 4)            52          reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_177 (Embedding)       (None, 1, 1)         1           input_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_178 (Embedding)       (None, 1, 1)         2           input_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_179 (Embedding)       (None, 1, 1)         3           input_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_34 (TensorFlowO [(None, 4)]          0           concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_18 (TensorFl [(None, 3, 4)]       0           concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 4)            16          dense_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 3, 1)         0           embedding_177[0][0]              \n",
      "                                                                 embedding_178[0][0]              \n",
      "                                                                 embedding_179[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_17 (TensorFl [(None, 4)]          0           tf_op_layer_Sum_34[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_35 (TensorFlowO [(None, 4)]          0           tf_op_layer_Square_18[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_65 (TensorFlow [(None, 4)]          0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_23 (TensorF [(None, 3)]          0           concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 4)            0           tf_op_layer_Square_17[0][0]      \n",
      "                                                                 tf_op_layer_Sum_35[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 4)            0           tf_op_layer_Relu_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_33 (TensorFlowO [(None,)]            0           tf_op_layer_Squeeze_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_36 (TensorFlowO [(None,)]            0           subtract_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 1)            5           dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None,)              0           tf_op_layer_Sum_33[0][0]         \n",
      "                                                                 tf_op_layer_Sum_36[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 1)            0           dense_89[0][0]                   \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1)            0           add_11[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 103\n",
      "Trainable params: 95\n",
      "Non-trainable params: 8\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = model(4, [1,2,3])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "Features:\n",
    "- user: over threshold, otherwise etc group\n",
    "- Documents: over threshold, otherwise eliminate from data\n",
    "- author: document author\n",
    "- doc_age:\n",
    "- tag\n",
    "- magazine_id\n",
    "- pop: how many times all other users read the doc\n",
    "- is_followed: is the auhtor followed by user\n",
    "\n",
    "Y: will like or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_documents(thresh=100):\n",
    "    doc_read = {}\n",
    "    path = \"{}/read/\".format(data_dir)\n",
    "    for dirpath, subdirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            filename = dirpath+f\n",
    "            file = open(filename, 'r')\n",
    "            for line in file.readlines():\n",
    "                words = line.strip().split(' ')\n",
    "                user = words[0]\n",
    "                for doc in words[1:]:\n",
    "                    if doc not in doc_read:\n",
    "                        doc_read[doc] = {}\n",
    "                        doc_read[doc]['num']  = 1\n",
    "                        doc_read[doc]['reader'] = [user]\n",
    "                    else:\n",
    "                        doc_read[doc]['num'] += 1\n",
    "                        doc_read[doc]['reader'].append(user)\n",
    "                        \n",
    "    doc_read_thresh = {key:{'num':doc_read[key]['num'], 'reader':doc_read[key]['reader']} for key in doc_read if doc_read[key]['num'] > thresh}\n",
    "    \"\"\"\n",
    "    total doc: 505840\n",
    "    doc over thresh=100: 36340\n",
    "    \"\"\"\n",
    "    return doc_read_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@charlessay_30': {'num': 1, 'reader': ['#a055d0c3520e1c002531001928217887']}, '@wal8am_27': {'num': 1, 'reader': ['#a055d0c3520e1c002531001928217887']}, '@uglyduckmin_40': {'num': 1, 'reader': ['#a055d0c3520e1c002531001928217887']}, '@anti-essay_133': {'num': 1, 'reader': ['#a055d0c3520e1c002531001928217887']}, '@roysday_125': {'num': 1, 'reader': ['#a055d0c3520e1c002531001928217887']}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_valid_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_list(thresh=100, etc_user_num=200):\n",
    "    valid_doc = get_valid_documents()\n",
    "    print(len(valid_doc))\n",
    "    user_read_num = {}\n",
    "    user_read_doc = {}\n",
    "    for doc in valid_doc:\n",
    "        readers = valid_doc[doc]['reader']\n",
    "        for reader in readers:\n",
    "            if reader not in user_read_doc:\n",
    "                user_read_doc[reader] = [doc]\n",
    "                user_read_num[reader] = 1\n",
    "            else:\n",
    "                user_read_doc[reader].append(doc)\n",
    "                user_read_num[reader] += 1\n",
    "    \n",
    "    user_read_num1 = {key:user_read_num[key] for key in user_read_num if user_read_num[key] >= thresh}\n",
    "    user_read_doc1 = {key:user_read_doc[key] for key in user_read_num1}\n",
    "    \n",
    "    user_read_num2 = {key:user_read_num[key] for key in user_read_num if user_read_num[key] < thresh}\n",
    "    user_read_num2 = {key:user_read_num2[key] for i, key in enumerate(user_read_num2) if i < etc_user_num}\n",
    "    user_read_doc2 = {key:user_read_doc[key] for key in user_read_num2}\n",
    "    return user_read_doc1, user_read_num1, user_read_doc2, user_read_num2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_meta_dic():\n",
    "    \"\"\"\n",
    "    document(id): author(user_id), tags(keyword_lists), magazine_id,  \n",
    "    \"\"\"\n",
    "    data = open('../data/metadata.json', 'r')\n",
    "    for line in data.readlines():\n",
    "        line = json.loads(line)\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'magazine_id': 8982, 'user_id': '@bookdb', 'title': '사진으로 옮기기에도 아까운, 리치필드 국립공원', 'keyword_list': ['여행', '호주', '국립공원'], 'display_url': 'https://brunch.co.kr/@bookdb/782', 'sub_title': '세상 어디에도 없는 호주 Top 10', 'reg_ts': 1474944427000, 'article_id': 782, 'id': '@bookdb_782'}\n"
     ]
    }
   ],
   "source": [
    "get_doc_meta_dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_value(target, cat_num=100):\n",
    "    new_target = {}\n",
    "    target_list = list(target.values())\n",
    "    max_ = max(target_list)\n",
    "    min_ = min(target_list)\n",
    "    division = int((max_ - min_ +1) / cat_num)\n",
    "    for key in pop:\n",
    "        for i in range(cat_num):\n",
    "            if target[key] >= (min_ + division*i) and target[key] < (min_+division*(i+1)):\n",
    "                new_target[key] = i+1\n",
    "        if target[key] >= min_ + division*cat_num:\n",
    "            new_target[key] = cat_num\n",
    "    return new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = {'a': 3, 'b': 4, 'c': 3, 'd':6, 'e': 7, 'f': 8, 'g':9}\n",
    "categorize_pop(pop, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_id2author_id(doc_id):\n",
    "    return doc_id.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_doc_read_num(user_id, doc_id, user_read_doc):\n",
    "    cnt = 0\n",
    "    docs_read = user_read_doc[user_id]\n",
    "    for doc in docs_read:\n",
    "        if doc == doc_id:\n",
    "            cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def combine_data_to_df(user_thresh=100, doc_thresh=100, pop_cat_num=100):\n",
    "    valid_doc = get_valid_documents(thresh=doc_thresh) # doc: read num over thresh\n",
    "    pop = categorize_value(valid_doc, cat_num=pop_cat_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
