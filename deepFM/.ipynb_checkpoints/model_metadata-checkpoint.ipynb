{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'user': ['Jason', 'Molly', 'Amy', 'Jake', 'Amy'],\n",
    "        'doc':['q', 'M', 'T', 'q', 'Ay'],\n",
    "        'author': ['Jason', 'Jason', 'Tina', 'Jason', 'Amy'],\n",
    "        'views': [4, 24, 3100, 2, 3],\n",
    "        'y': [1,1,3,1,1]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(embedding_size, field_vocab_size=[], hidden_units=[4,4,4], dropout=0.5):\n",
    "    F = len(field_vocab_size)\n",
    "    \n",
    "    # prepare embeddings\n",
    "    inputs = []\n",
    "    embed_list = [] \n",
    "    for i, vocab_size in enumerate(field_vocab_size):\n",
    "        in_ = keras.Input(shape=(1,))\n",
    "        inputs.append(in_)\n",
    "        embed_list.append(layers.Embedding(vocab_size, embedding_size)(in_))\n",
    "    embed_list = layers.concatenate(embed_list, axis=1) # none, F, K\n",
    "    \n",
    "    fm_one_inputs = []\n",
    "    embed_one_list = [] # none, F, 1\n",
    "    for i, vocab_size in enumerate(field_vocab_size):\n",
    "        in_ = keras.Input(shape=(1,))\n",
    "        inputs.append(in_)\n",
    "        embed_one_list.append(layers.Embedding(vocab_size, 1)(in_))\n",
    "    fm_first_in = layers.concatenate(embed_one_list, axis=1)\n",
    "    fm_first_in = backend.squeeze(fm_first_in, axis=2) # none, F\n",
    "    \n",
    "    # dense layer\n",
    "    dropouts = [dropout] * len(hidden_units)\n",
    "    weight_init = keras.initializers.glorot_uniform()\n",
    "    \n",
    "    deep_in = layers.Reshape((F*embedding_size,))(embed_list)\n",
    "    for i, (h, d) in enumerate(zip(hidden_units, dropouts)):\n",
    "        z = layers.Dense(units=h, kernel_initializer=weight_init)(deep_in)\n",
    "        z = layers.BatchNormalization(axis=-1)(z)\n",
    "        z = keras.activations.relu(z)\n",
    "        z = layers.Dropout(d,seed=d * i)(z) if d > 0 else z\n",
    "    deep_out = layers.Dense(units=1, activation=tf.nn.softmax, kernel_initializer=weight_init)(z)\n",
    "    # deep_out: None, 1\n",
    "    \n",
    "    # fm layer\n",
    "    fm_first_order = backend.sum(fm_first_in, axis=1) #None, 1\n",
    "    \n",
    "    emb_sum_squared = backend.square(backend.sum(embed_list, axis=1)) #none, K\n",
    "    emb_squared_sum = backend.sum(backend.square(embed_list), axis=1) #none, K\n",
    "    fm_second_order = layers.Subtract()([emb_sum_squared, emb_squared_sum])\n",
    "    fm_second_order = backend.sum(fm_second_order, axis=1) #none, 1\n",
    "    fm_out = layers.Add()([fm_first_order, fm_second_order])\n",
    "    \n",
    "    out = layers.Add()([deep_out, fm_out])\n",
    "    out = layers.Activation(activation='sigmoid')(out)\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_175 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_176 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_177 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_174 (Embedding)       (None, 1, 4)         4           input_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_175 (Embedding)       (None, 1, 4)         8           input_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_176 (Embedding)       (None, 1, 4)         12          input_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 3, 4)         0           embedding_174[0][0]              \n",
      "                                                                 embedding_175[0][0]              \n",
      "                                                                 embedding_176[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 12)           0           concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_178 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_179 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_180 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 4)            52          reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_177 (Embedding)       (None, 1, 1)         1           input_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_178 (Embedding)       (None, 1, 1)         2           input_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_179 (Embedding)       (None, 1, 1)         3           input_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_34 (TensorFlowO [(None, 4)]          0           concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_18 (TensorFl [(None, 3, 4)]       0           concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 4)            16          dense_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 3, 1)         0           embedding_177[0][0]              \n",
      "                                                                 embedding_178[0][0]              \n",
      "                                                                 embedding_179[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_17 (TensorFl [(None, 4)]          0           tf_op_layer_Sum_34[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_35 (TensorFlowO [(None, 4)]          0           tf_op_layer_Square_18[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_65 (TensorFlow [(None, 4)]          0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_23 (TensorF [(None, 3)]          0           concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 4)            0           tf_op_layer_Square_17[0][0]      \n",
      "                                                                 tf_op_layer_Sum_35[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 4)            0           tf_op_layer_Relu_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_33 (TensorFlowO [(None,)]            0           tf_op_layer_Squeeze_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_36 (TensorFlowO [(None,)]            0           subtract_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 1)            5           dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None,)              0           tf_op_layer_Sum_33[0][0]         \n",
      "                                                                 tf_op_layer_Sum_36[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 1)            0           dense_89[0][0]                   \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1)            0           add_11[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 103\n",
      "Trainable params: 95\n",
      "Non-trainable params: 8\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = model(4, [1,2,3])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "Features:\n",
    "- user: over threshold, otherwise etc group\n",
    "- Documents: over threshold, otherwise eliminate from data\n",
    "- author: document author\n",
    "- doc_age:\n",
    "- tag\n",
    "- magazine_id\n",
    "- pop: how many times all other users read the doc\n",
    "- is_followed: is the auhtor followed by user\n",
    "\n",
    "Y: will like or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_documents(thresh=100):\n",
    "    doc_read = {}\n",
    "    path = \"{}/read/\".format(data_dir)\n",
    "    for dirpath, subdirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            filename = dirpath+f\n",
    "            file = open(filename, 'r')\n",
    "            for line in file.readlines():\n",
    "                words = line.strip().split(' ')\n",
    "                for word in words[1:]:\n",
    "                    if word not in doc_read:\n",
    "                        doc_read[word]  = 1\n",
    "                    else:\n",
    "                        doc_read[word] += 1\n",
    "    doc_read_thresh = {key:doc_read[key] for key in doc_read if doc_read[key] > thresh}\n",
    "    \"\"\"\n",
    "    total doc: 505840\n",
    "    doc over thresh=100: 36340\n",
    "    \"\"\"\n",
    "    return doc_read_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-123aa185a16a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_valid_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-141-672035fa7de9>\u001b[0m in \u001b[0;36mget_valid_documents\u001b[0;34m(thresh)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mdoc_read\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_valid_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_list(thresh=100, etc_user_num=200):\n",
    "    valid_doc = get_valid_documents()\n",
    "    users = {}\n",
    "    path = \"{}/read/\".format(data_dir)\n",
    "    \"\"\"\n",
    "    files length: 3624\n",
    "    \"\"\"\n",
    "    for dirpath, subdirs, files in os.walk(path):\n",
    "        for i, f in enumerate(files):\n",
    "            filename = dirpath+f\n",
    "            file = open(filename, 'r')\n",
    "            for line in file.readlines():\n",
    "                words = line.strip().split(' ')\n",
    "                if words[0] not in users:\n",
    "                    users[words[0]] = [doc for doc in words[1:] if doc in valid_doc]\n",
    "                else:\n",
    "                    users[words[0]] +=[doc for doc in words[1:] if doc in valid_doc]\n",
    "            if i% 10 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    etc_users = {key:users[key] for key in users if len(users[key]) < thresh}\n",
    "    users_after_removal = {key:users[key] for key in users if len(users[key]) >= thresh}\n",
    "    \n",
    "    keys = random.sample(list(etc_users.keys()), 200)\n",
    "    values = [d[k] for k in keys]z\n",
    "    etc_users = {keys[k]:values[k] for k in range(len(keys))}\n",
    "    \n",
    "    >>> d = dict.fromkeys(range(100))\n",
    ">>> keys = random.sample(list(d), 10)\n",
    ">>> keys\n",
    "[52, 3, 10, 92, 86, 42, 99, 73, 56, 23]\n",
    ">>> values = [d[k] for k in keys]\n",
    "    print(len(users))\n",
    "    print(len(etc_users))\n",
    "    print(len(users_after_removal))\n",
    "    #return etc_users, users_after_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3624\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-7b49c7896e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_user_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-202-af71a14973e4>\u001b[0m in \u001b[0;36mget_user_list\u001b[0;34m(thresh)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-af71a14973e4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_user_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_meta_dic():\n",
    "    \"\"\"\n",
    "    {document(id): { tags(keyword_list),age(unix timestamp) ,magazine_id,}}  \n",
    "    \"\"\"\n",
    "    valid_doc = get_valid_documents().keys()\n",
    "\n",
    "    data = open('../data/metadata.json', 'r')\n",
    "    meta={}\n",
    "    for line in data.readlines():\n",
    "        if line['id'] in valid_doc:\n",
    "            line = json.loads(line)\n",
    "            tmp_dict={}\n",
    "            tmp_dict[line['id']]= {'keyword_list':line['keyword_list'],\n",
    "            'mag_id':line['magazine_id'],\n",
    "            'reg_ts':line['reg_ts']\n",
    "            }\n",
    "            meta.update(tmp_dict)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_value(target, cat_num=100):\n",
    "    new_target = {}\n",
    "    target_list = list(target.values())\n",
    "    max_ = max(target_list)\n",
    "    min_ = min(target_list)\n",
    "    division = int((max_ - min_ +1) / cat_num)\n",
    "    for key in pop:\n",
    "        for i in range(cat_num):\n",
    "            if target[key] >= (min_ + division*i) and target[key] < (min_+division*(i+1)):\n",
    "                new_target[key] = i+1\n",
    "        if target[key] >= min_ + division*cat_num:\n",
    "            new_target[key] = cat_num\n",
    "    return new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "3\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 1, 'c': 1, 'd': 2, 'e': 3, 'f': 3, 'g': 3}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = {'a': 3, 'b': 4, 'c': 3, 'd':6, 'e': 7, 'f': 8, 'g':9}\n",
    "categorize_pop(pop, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def comibine_data_to_df(user_thresh=100, doc_thresh=100, pop_cat_num=100):\n",
    "    valid_doc = get_valid_documents(thresh=doc_thresh) # doc: read num over thresh\n",
    "    pop = categorize_value(valid_doc, cat_num=pop_cat_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_user=get_user_list(100,200)[0]\n",
    "etc_user=get_user_list(100,200)[2]\n",
    "\n",
    "user_list=valid_user.update(etc_user)\n",
    "def is_followed(user_list=user_list,author):\n",
    "    data = open('../data/users.json', 'r')\n",
    "    for line in data.readlines():\n",
    "        line=json.loads(line)\n",
    "        if line['id'] in user_list.keys():\n",
    "            if author in line['following_list']:\n",
    "                return 1\n",
    "            else: return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
